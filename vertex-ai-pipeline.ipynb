{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd83bf3-97a7-4839-b94c-f49ce5b4878a",
   "metadata": {},
   "source": [
    "# How to use Google Cloud Vertex AI to build a ML pipeline usingÂ Kubeflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81208cdf",
   "metadata": {},
   "source": [
    "Hello everyone! Welcome to the Step by Step tutorial to run your first model using Kubeflow pipelines. To know more details about each step we recommend you to read our [medium article]() and for a simpler version you can also check the other article that we developed in [here](https://medium.com/@outsidenoxvodafone/running-your-first-ml-model-using-gcp-on-vertex-ai-1535b6732c6c)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20150ed-0d21-4d6f-b037-e5a679449ee2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Step 0: Download the required packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345d672d-2076-4aaf-8e56-2f920526f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp==1.8.14\n",
    "!pip install google-cloud-aiplatform==1.18.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527aeacd-37e3-46c6-9e83-4174dab86778",
   "metadata": {},
   "source": [
    "### **Step 1: Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56e1c30f-2195-402f-833f-674b42b9a274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import NamedTuple\n",
    "from datetime import datetime\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "from kfp.v2.dsl import (compiler, component, Input, Model, Output, Dataset, Artifact, \n",
    "                        OutputPath, ClassificationMetrics, Metrics, InputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8e7822-e30f-4920-bbd2-7e29f94a592e",
   "metadata": {},
   "source": [
    "### **Step 2: Define variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d990ab9-c943-4855-8899-c235db0f76b9",
   "metadata": {},
   "source": [
    "* Define the pipeline varibles to setup the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28dccd57-3888-417f-a15d-d7edef5f97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "PROJECT_ID = #you have to fill in your project id \n",
    "LOCATION = #you have to fill in the location of the data\n",
    "PIPELINE_ROOT = #the location where the pipeline's artifacts are stored\n",
    "SERVICE_ACCOUNT = #the service account to connect with your project\n",
    "PIPELINE_NAME = \"medium-article\"\n",
    "JOBID = f\"training-pipeline-{TIMESTAMP}\"\n",
    "ENABLE_CACHING = False\n",
    "TEMPLATE_PATH = \"medium_pipeline.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d325311-08b6-4031-813a-35ace1374bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"medium_article\"\n",
    "TABLE_ID = \"temp_table_medium\"\n",
    "COL_LABEL = \"class\" \n",
    "COL_TRAINING=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "PIPELINE_PARAMS = {\"project_id\": PROJECT_ID,\n",
    "                   \"dataset_location\": LOCATION,\n",
    "                   \"table_id\": TABLE_ID,\n",
    "                   \"dataset_id\": DATASET_ID,\n",
    "                   \"col_label\": COL_LABEL,\n",
    "                   \"col_training\": COL_TRAINING}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a3218-e2db-4727-9674-09b01c7b605e",
   "metadata": {},
   "source": [
    "* Define the docker image versions, to get other images go to: https://cloud.google.com/deep-learning-containers/docs/choosing-container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feb3f517-054b-44e3-a611-e8ef1720e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = \"gcr.io/deeplearning-platform-release/xgboost-cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4163573-e832-42a4-8f03-75bfb2395293",
   "metadata": {},
   "source": [
    "* Define your packages versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b176fbef-1ce6-4c4f-9181-2489231c322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_BIGQUERY = \"google-cloud-bigquery==2.30.0\"\n",
    "PANDAS = \"pandas==1.3.2\"\n",
    "SKLEARN = \"scikit-learn==1.0.2\"\n",
    "NUMPY = \"numpy==1.21.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef87a1d-cf9a-446a-baaf-ef10df14cc7f",
   "metadata": {},
   "source": [
    "### **Step 3: Building the components**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d954d36-de70-404e-9992-fcb3120ab1a4",
   "metadata": {},
   "source": [
    "The next step is to build each component in separate, where each one have a specific utility and meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c315ffe-2b95-41d5-a852-6afd39a16ef2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. INGEST DATA FROM BIG QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23c07f67-7f88-44b6-b6cb-5206793b853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE, packages_to_install=[GCP_BIGQUERY])\n",
    "def query_to_table(\n",
    "    query: str,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    location: str = \"EU\",\n",
    "    query_job_config: dict = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Run the query and create a new BigQuery table\n",
    "    \"\"\"\n",
    "    \n",
    "    import google.cloud.bigquery as bq\n",
    "    \n",
    "    # Configure your query job\n",
    "    job_config = bq.QueryJobConfig(destination=f\"{project_id}.{dataset_id}.{table_id}\", \n",
    "                                   **query_job_config)\n",
    "    \n",
    "    # Initiate the Big Query client to connect with the project\n",
    "    bq_client = bq.Client(project=project_id, \n",
    "                          location=location)\n",
    "    \n",
    "    # Generate the query with all the job configurations\n",
    "    query_job = bq_client.query(query, \n",
    "                                job_config=job_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff74a6f-329c-4525-aa2e-06111cd81b8a",
   "metadata": {},
   "source": [
    "#### 2. INGEST DATA FROM BIG QUERY to GOOGLE CLOUD STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d59f5d-10c4-432f-9733-22a868000955",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE, packages_to_install=[GCP_BIGQUERY])\n",
    "def extract_table_to_gcs(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    dataset: Output[Dataset],\n",
    "    location: str = \"EU\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Extract a Big Query table into Google Cloud Storage.\n",
    "    \"\"\"\n",
    "\n",
    "    import logging\n",
    "    import os\n",
    "    import google.cloud.bigquery as bq\n",
    "\n",
    "    # Get the table generated on the previous component\n",
    "    full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    table = bq.table.Table(table_ref=full_table_id)\n",
    "\n",
    "    # Initiate the Big Query client to connect with the project\n",
    "    job_config = bq.job.ExtractJobConfig(**{})\n",
    "    client = bq.client.Client(project=project_id, location=location)\n",
    "\n",
    "    # Submit the extract table job to store on GCS\n",
    "    extract_job = client.extract_table(table, dataset.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711dbbe-71b8-4af3-971a-4b7fe05ccade",
   "metadata": {},
   "source": [
    "#### 3. CREATE THE TRAINING AND TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93fde602-f464-447c-bb31-1646496910d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE, packages_to_install=[PANDAS, SKLEARN]\n",
    ")\n",
    "def create_sets(\n",
    "    data_input: Input[Dataset],\n",
    "    dataset_train: OutputPath(),\n",
    "    dataset_test: OutputPath(),\n",
    "    col_label: str,\n",
    "    col_training: list\n",
    "    ) -> NamedTuple(\"Outputs\", [(\"dict_keys\", dict), (\"shape_train\", int), (\"shape_test\", int)]):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Split data into train and test set.\n",
    "    \"\"\"\n",
    "\n",
    "    import logging\n",
    "    import pickle\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn import model_selection\n",
    "    \n",
    "\n",
    "    def convert_labels_to_categories(labels):\n",
    "        \"\"\"\n",
    "        Function returns a dictionary with the encoding of labels.\n",
    "        :returns: A Pandas DataFrame with all the metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dic_keys = {k: label for k, label in enumerate(sorted(labels.unique()))}\n",
    "            dic_vals = {label: k for k, label in enumerate(sorted(labels.unique()))}\n",
    "            return dic_vals, dic_keys\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] Something went wrong that is {e}')\n",
    "        return {}, {}\n",
    "\n",
    "\n",
    "    df_ = pd.read_csv(data_input.path)\n",
    "    \n",
    "    df_.dropna(inplace=True)\n",
    "\n",
    "    logging.info(f\"[START] CREATE SETS, starts with an initial shape of {df_.shape}\")\n",
    "\n",
    "    if len(df_) != 0:\n",
    "\n",
    "        yy = df_[col_label]\n",
    "        dic_vals, dic_keys = convert_labels_to_categories(yy)\n",
    "\n",
    "        yy = yy.apply(lambda v: dic_vals[v])\n",
    "        xx = df_[col_training]\n",
    "\n",
    "        x_train, x_test, y_train, y_test = model_selection.train_test_split(xx, yy, test_size=0.2, random_state=0, stratify=yy)\n",
    "\n",
    "        x_train_results = {'x_train': x_train, 'y_train': y_train}\n",
    "        x_test_results = {'x_test': x_test, 'y_test': y_test}\n",
    "\n",
    "        with open(dataset_train + f\".pkl\", 'wb') as file:\n",
    "            pickle.dump(x_train_results, file)\n",
    "\n",
    "        with open(dataset_test + \".pkl\", 'wb') as file:\n",
    "            pickle.dump(x_test_results, file)\n",
    "\n",
    "        logging.info(f\"[END] CREATE SETS, dataset was split\")\n",
    "\n",
    "        return (dic_keys, len(x_train), len(x_test))\n",
    "\n",
    "    else:\n",
    "        logging.error(f\"[END] CREATE SETS, dataset is empty\")\n",
    "        return (None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49a968-4a7e-4948-a188-87ea895713ec",
   "metadata": {},
   "source": [
    "#### 4. LET'S TRAIN MODEL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced4f8b-b14b-4304-bafd-f9bafe3886c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE, packages_to_install=[SKLEARN, PANDAS]\n",
    ")\n",
    "def train_model(\n",
    "    training_data: InputPath(),\n",
    "    model: Output[Model],\n",
    ") -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Train a classification model.\n",
    "    \"\"\"\n",
    "        \n",
    "    import logging\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    with open(training_data + \".pkl\", 'rb') as file:\n",
    "        train_data = pickle.load(file)\n",
    "\n",
    "    X_train = train_data['x_train']\n",
    "    y_train = train_data['y_train']\n",
    "    \n",
    "    logging.info(f\"X_train shape {X_train.shape}\")\n",
    "    logging.info(f\"y_train shape {y_train.shape}\")\n",
    "    \n",
    "    logging.info(\"Starting Training...\")\n",
    "\n",
    "    clf = LogisticRegression(n_jobs=-1, random_state=42)\n",
    "    train_model = clf.fit(X_train, y_train)\n",
    "\n",
    "    # ensure to change GCS to local mount path\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "\n",
    "    logging.info(f\"Save model to: {model.path}\")\n",
    "    joblib.dump(train_model, model.path + \"/model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189b086e-741b-4705-b86c-f96044d7613e",
   "metadata": {},
   "source": [
    "#### 5. CREATE SOME PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ff82b-57a7-4696-8400-aee5e42fc2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE, packages_to_install=[PANDAS]\n",
    ")\n",
    "def predict_model(\n",
    "    test_data: InputPath(),\n",
    "    model: Input[Model],\n",
    "    predictions: Output[Dataset],\n",
    ") -> None:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Create the predictions of the model.\n",
    "    \"\"\"    \n",
    "\n",
    "    import logging\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "    with open(test_data + \".pkl\", 'rb') as file:\n",
    "        test_data = pickle.load(file)\n",
    "\n",
    "    X_test = test_data['x_test']\n",
    "    y_test = test_data['y_test']\n",
    "\n",
    "    # load model\n",
    "    model_path = os.path.join(model.path, \"model.joblib\")\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # predict and save to prediction column\n",
    "    df = pd.DataFrame({\n",
    "        'input': X_test.tolist(),\n",
    "        'class_true': y_test.tolist(),\n",
    "        'class_pred': y_pred.tolist()}\n",
    "    )\n",
    "\n",
    "    # save dataframe (feature, labels if provided, predictions)\n",
    "    df.to_csv(predictions.path, sep=\",\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e82e2-1734-48a2-a361-fe01ad6e2f42",
   "metadata": {},
   "source": [
    "#### 6. CREATE EVALUATION METRICS COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aec453-a5db-4ead-9c00-46c33f89cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=BASE_IMAGE, packages_to_install=[PANDAS, NUMPY]\n",
    ")\n",
    "\n",
    "def evaluation_metrics(\n",
    "    predictions: Input[Dataset],\n",
    "    metrics_names: list,\n",
    "    dict_keys: dict,\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    kpi: Output[Metrics],\n",
    "    eval_metrics: Output[Metrics]\n",
    ") -> None:\n",
    "    \n",
    "    \"\"\"\n",
    "    Create the evaluation metrics.\n",
    "    \"\"\" \n",
    "\n",
    "    import json\n",
    "    import logging\n",
    "    from importlib import import_module\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    results = pd.read_csv(predictions.path)\n",
    "    \n",
    "    results['class_true_clean'] = results[true_column_name].astype(str).map(dict_keys)\n",
    "    results['class_pred_clean'] = results[pred_column_name].astype(str).map(dict_keys)\n",
    "\n",
    "    module = import_module(f\"sklearn.metrics\")\n",
    "    metrics_dict = {}\n",
    "\n",
    "    for each_metric in metrics_names:\n",
    "        metric_func = getattr(module, each_metric)\n",
    "\n",
    "        if each_metric == 'f1_score':\n",
    "            metric_val = metric_func(results['class_true'], results['class_pred'], average=None)\n",
    "        else:\n",
    "            metric_val = metric_func(results['class_true'], results['class_pred'])\n",
    "\n",
    "        # Save metric name and value\n",
    "        metric_val = np.round(np.average(metric_val), 4)\n",
    "        metrics_dict[f\"{each_metric}\"] = metric_val\n",
    "        kpi.log_metric(f\"{each_metric}\", metric_val)\n",
    "        \n",
    "        # dumping kpi metadata\n",
    "        with open(kpi.path, \"w\") as f:\n",
    "            json.dump(kpi.metadata, f)\n",
    "        logging.info(f\"{each_metric}: {metric_val:.3f}\")\n",
    "\n",
    "    # dumping metrics_dict\n",
    "    with open(eval_metrics.path, \"w\") as f:\n",
    "        json.dump(metrics_dict, f)\n",
    "\n",
    "    confusion_matrix_func = getattr(module, \"confusion_matrix\")\n",
    "    metrics.log_confusion_matrix(list(dict_keys.values()),\n",
    "        confusion_matrix_func(results['class_true_clean'], results['class_pred_clean']).tolist(),)\n",
    "\n",
    "    # dumping metrics metadata\n",
    "    with open(metrics.path, \"w\") as f:\n",
    "        json.dump(metrics.metadata, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1052deae-d053-462a-9249-785b08f07dfa",
   "metadata": {},
   "source": [
    "### **Step 4: Build the kubeflow pipeline with all the components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5172eab-78ef-47ea-ba03-2ec803f252b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT)\n",
    "def medium_pipeline(\n",
    "    project_id: str,\n",
    "    dataset_location: str,\n",
    "    dataset_id: str,\n",
    "    table_id: str,\n",
    "    col_label: str,\n",
    "    col_training: list):\n",
    "    \n",
    "    QUERY = \"\"\"SELECT * FROM `project-id.medium_article.iris_dataset`\"\"\"\n",
    "    METRICS_NAMES = [\"accuracy_score\", \"f1_score\"]\n",
    "    \n",
    "    ingest = query_to_table(query=QUERY,\n",
    "                            table_id=table_id,\n",
    "                            project_id=project_id,\n",
    "                            dataset_id=dataset_id,\n",
    "                            location=dataset_location,\n",
    "                            query_job_config=json.dumps(dict(write_disposition=\"WRITE_TRUNCATE\"))\n",
    "                           ).set_display_name(\"Ingest Data\")\n",
    "    \n",
    "    # From big query store in GCS\n",
    "    ingested_dataset = (\n",
    "                        extract_table_to_gcs(\n",
    "                            project_id=project_id,\n",
    "                            dataset_id=dataset_id,\n",
    "                            table_id=table_id,\n",
    "                            location=dataset_location,\n",
    "                        )\n",
    "                        .after(ingest)\n",
    "                        .set_display_name(\"Extract Big Query to GCS\")\n",
    "                    )\n",
    "    \n",
    "    # Split data\n",
    "    spit_data = create_sets(data_input=ingested_dataset.outputs[\"dataset\"],\n",
    "                              col_label=col_label,\n",
    "                              col_training=col_training\n",
    "                           ).set_display_name(\"Split data\")\n",
    "    \n",
    "    # Train model\n",
    "    training_model = train_model(\n",
    "        training_data=spit_data.outputs['dataset_train']).set_display_name(\"Train Model\")\n",
    "    \n",
    "    # Predit model\n",
    "    predict_data = predict_model(\n",
    "                test_data=spit_data.outputs['dataset_test'],\n",
    "                model=training_model.outputs[\"model\"]\n",
    "            ).set_display_name(\"Create Predictions\")\n",
    "    \n",
    "    \n",
    "    # Evaluate model\n",
    "    eval_metrics = evaluation_metrics(\n",
    "        predictions=predict_data.outputs['predictions'],\n",
    "        dict_keys=spit_data.outputs['dict_keys'],\n",
    "        metrics_names=json.dumps(METRICS_NAMES),\n",
    "        ).set_display_name(\"Evaluation Metrics\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f02d3-10ff-4de4-9f98-05037a2e328f",
   "metadata": {},
   "source": [
    "### **Step 5: Compile the Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0fbc5a3-9ff9-4b10-9976-9cef39433175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=medium_pipeline,\n",
    "    package_path=TEMPLATE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffa7c47-fa41-4434-be80-1d65ace539d0",
   "metadata": {},
   "source": [
    "### **Step 6: Let's submit this pipeline!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d5d78c-715b-4b97-a784-ed3e9102b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e9f2f6e-9012-4b5c-bd35-61de32164779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/737341566193/locations/europe-west1/pipelineJobs/training-pipeline-20221117093807\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/737341566193/locations/europe-west1/pipelineJobs/training-pipeline-20221117093807')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/training-pipeline-20221117093807?project=737341566193\n"
     ]
    }
   ],
   "source": [
    "pipeline_ = aiplatform.pipeline_jobs.PipelineJob(\n",
    "    enable_caching=ENABLE_CACHING,\n",
    "    display_name=PIPELINE_NAME,\n",
    "    template_path=TEMPLATE_PATH,\n",
    "    job_id=JOBID,\n",
    "    parameter_values=PIPELINE_PARAMS)\n",
    "\n",
    "pipeline_.submit(service_account=SERVICE_ACCOUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
